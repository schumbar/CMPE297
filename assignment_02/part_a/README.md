# Assignment 02 Part A - LLama Factory

## Assignment Description

Submit colabs for the following tasks:
a. Clone the colabs and execute it in your environment.
b. Submit a 1 minute video of what the colab does. This should be a video of you executing the colab.
    - Note: be sure to use the T4 GPU and smallest models possible (i.e. gemma-2b-quanitatized) as larger ones may consume too many resources.
c. You must demonstrate the following:
    - Llama-Factory - Demonstrate Qlora or Lora.
    - Demonstrate supervised fine-tuning (Either LORA or QLORA is fine)
    - DPO Training
    - PPO Training

## Assignment Deliverables

### Part A Deliverables

Please see below for the list of deliverables that have been submitted for this assignment. Please note that all deliverables are under the `assignment_02/part_a` folder.

1. `README.md`: A Markdown file containing the description of the deliverables Assignment 02 Part A.
2. `ShawnChumbar_Assignment02_PartA_Fintetuning.ipynb`: A Google Colab notebook containing the code for fine-tuning of the **google/gemma-2b** model.
3. `ShawnChumbar_Assignment02_PartA_DPO_Training.ipynb`: A Google Colab notebook containing the code for the DPO training portion of assignment02 part A.
4. `ShawnChumbar_Assignment02_PartA_PPO_Training.ipynb`: A Google Colab notebook containing the code for the PPO Training portion of assignment02 part A.

### References

1. [LLaMA-Factory on GitHub](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file)
2. [LLaMA-Factory Colab Notebook](https://colab.research.google.com/drive/1fvw1MR3o-03qQ9eRw09glkN2VqIybKNm?usp=sharing)
3. [LLaMA-Factory on PyPI](https://pypi.org/project/llamafactory/)
4. [Youtube Video: Direct Preference Optimization: Your Language Model is Secretly a Reward Model | DPO paper explained](https://www.youtube.com/watch?v=XZLc09hkMwA&ab_channel=AICoffeeBreakwithLetitia)
